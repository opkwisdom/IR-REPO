training:
  batch_size: 128
  per_device_batch_size: 16
  gradient_accumulation_steps: 4

  learning_rate: 1e-5
  weight_decay: 0.01
  max_epochs: 5

model:
  model_name_or_path: bert-base-uncased
  use_gradient_checkpointing: false